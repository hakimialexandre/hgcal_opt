{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date='16Mar2021'\n",
    "dir_name='/preprocessing/200PU_2806'\n",
    "workdir=os.getcwd()\n",
    "files_dir='/data_cms_upgrade/hakimi'\n",
    "os.makedirs(workdir+dir_name, exist_ok=True)\n",
    "data_dir=workdir+dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 3.41 s, total: 18.9 s\n",
      "Wall time: 19 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "columns=['genpart_exeta',\n",
    "         'genpart_pt',\n",
    " 'cl3d_pt',\n",
    " 'cl3d_eta',\n",
    " 'cl3d_showerlength',\n",
    " 'cl3d_coreshowerlength',\n",
    " 'cl3d_firstlayer',\n",
    " 'cl3d_maxlayer',\n",
    " 'cl3d_seetot',\n",
    " 'cl3d_seemax',\n",
    " 'cl3d_spptot',\n",
    " 'cl3d_sppmax',\n",
    " 'cl3d_szz',\n",
    " 'cl3d_srrtot',\n",
    " 'cl3d_srrmax',\n",
    " 'cl3d_srrmean',\n",
    " 'cl3d_emaxe',\n",
    " 'cl3d_hoe',\n",
    " 'cl3d_meanz',\n",
    " 'cl3d_layer10',\n",
    " 'cl3d_layer50',\n",
    " 'cl3d_layer90',\n",
    " 'cl3d_ntc67',\n",
    " 'cl3d_ntc90',\n",
    " 'layer',\n",
    " 'sample',\n",
    "  'matches',\n",
    "         \n",
    "        ]\n",
    "df= pd.read_csv(data_dir+'/cl3d.csv', usecols=columns, low_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/exp_soft/llr/python/3.7.0/el7/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/exp_soft/llr/python/3.7.0/el7/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/opt/exp_soft/llr/python/3.7.0/el7/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df['genpart_exeta'][df['sample']=='PU']=0\n",
    "df['genpart_pt'][df['sample']=='PU']=0\n",
    "df['matches'][df['sample']=='PU']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['genpart_pt'][df['sample']=='PU'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elec 279928\n",
      "pion 9545\n",
      "PU 29757\n"
     ]
    }
   ],
   "source": [
    "for samp in ['elec', 'pion', 'PU']:\n",
    "    print(samp,len(df[df['sample']==samp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genptcut=20\n",
    "cl3dptcut=5\n",
    "PU_cut = 20\n",
    "etamin=1.6\n",
    "etamax=2.9\n",
    "bkg='PU' # 'pions' or 'PU'\n",
    "standardize = \"_unstandardized\"\n",
    "feature_set=1 # 1: baseline 4: minimal, 10-13 + new vars (12 -> PU 13-> pions)\n",
    "opti=False\n",
    "res_dir=workdir+'/res/res_pt{}_vs{}_featset{}{}'.format(genptcut, bkg, feature_set, standardize)\n",
    "os.makedirs(res_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319230\n",
      "elec and pions pt cut : 314710 275772\n",
      "PU pt cut : 314710 275772\n",
      "min eta cut : 264678 256235\n",
      "max eta cut :  232315 225066\n",
      "drpna :  232315\n",
      "cl3d eta cut: 231661 224441\n",
      "done\n",
      "done layering36\n",
      "CPU times: user 2min 58s, sys: 14.3 s, total: 3min 12s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#cut\n",
    "algo_cut={}\n",
    "\n",
    "def tolist(x):\n",
    "    if x.matches==True:\n",
    "        x.test=x.layer[1:-1].split(',')\n",
    "    else: x.test=np.nan\n",
    "    return x.test\n",
    "print(len(df))\n",
    "\n",
    "#cut on pt gen level for electrons and pions\n",
    "sel=((df['genpart_pt']>genptcut) & (df['sample']!='PU')) | (df['sample']=='PU')\n",
    "df_cut=df[sel]\n",
    "\n",
    "#cut on cl3d_pt for elec and pions\n",
    "sel=((df['cl3d_pt'] > cl3dptcut) & (df['sample']!='PU')) | (df['sample']=='PU')\n",
    "df_cut=df[sel]\n",
    "print('elec and pions pt cut :',len(df_cut), len(df_cut[df_cut['sample']=='elec']))\n",
    "\n",
    "#cut on cl3d_pt for PU\n",
    "sel=((df_cut['cl3d_pt'] > PU_cut) & (df_cut['sample']=='PU')) | (df_cut['sample']!='PU')\n",
    "df_cut=df_cut[sel]\n",
    "print('PU pt cut :',len(df_cut), len(df_cut[df_cut['sample']=='elec']))\n",
    "\n",
    "#cut on hgcal eta acceptance\n",
    "sel=np.abs(df_cut['genpart_exeta'])>etamin\n",
    "df_cut=df_cut[sel]\n",
    "print('min eta cut :',len(df_cut), len(df_cut[df_cut['sample']=='elec']))\n",
    "sel=np.abs(df_cut['genpart_exeta'])<etamax\n",
    "df_cut=df_cut[sel]\n",
    "print('max eta cut : ',len(df_cut), len(df_cut[df_cut['sample']=='elec']))\n",
    "#drop incomplete events\n",
    "df_cut.dropna(inplace=True)\n",
    "print('drpna : ', len(df_cut))\n",
    "\n",
    "#cut on cl3deta\n",
    "df_cut['abseta']=np.abs(df_cut['cl3d_eta'])\n",
    "sel=(df_cut['abseta']>etamin) & (df_cut['abseta']<etamax)\n",
    "df_cut=df_cut[sel]\n",
    "print('cl3d eta cut:', len(df_cut), len(df_cut[df_cut['sample']=='elec']))\n",
    "\n",
    "#layer_pt preproc\n",
    "df_cut['layer_pt']=df_cut.apply(tolist, axis=1)\n",
    "df_cut.drop('layer', axis=1, inplace=True)\n",
    "print(\"done\")\n",
    "def layering (x):\n",
    "    return float(x.layer_pt[n])\n",
    "\n",
    "##besoin de crÃ©er une variable par layer?\n",
    "algo_layer={}\n",
    "n_layers=len(df_cut['layer_pt'].iloc[0])\n",
    "#print(n_layers)\n",
    "layer_columns=[]\n",
    "\n",
    "for n in range(n_layers):\n",
    "    print('layering: {}/{}\\r'.format(n+1,n_layers),end='', flush=True)\n",
    "    df_cut['layer_{}'.format(n)]=df_cut.apply(layering, axis=1)\n",
    "    layer_columns.append('layer_{}'.format(n))\n",
    "print(\"done layering\")\n",
    "\"\"\"if feature_set in [6]:\n",
    "    for n in range(n_layers):\n",
    "        print('layering: {}/{}\\r'.format(n+1,n_layers),end='', flush=True)\n",
    "        df_cut['layer_{}'.format(n)]=df_cut.apply(layering, axis=1)\n",
    "        df_cut['layer_{}'.format(n)] = df_cut['layer_{}'.format(n)]/df_cut['cl3d_pt']\n",
    "        layer_columns.append('layer_{}'.format(n))\n",
    "    print(\"done layering\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build new vars\n",
    "\n",
    "#abseta\n",
    "df_cut['abseta']=np.abs(df_cut['cl3d_eta'])\n",
    "\n",
    "#variance\n",
    "df_cut['varee']=df_cut['cl3d_seetot']**2\n",
    "df_cut['varpp']=df_cut['cl3d_spptot']**2\n",
    "df_cut['varzz']=df_cut['cl3d_szz']**2\n",
    "df_cut['varrr']=df_cut['cl3d_srrtot']**2\n",
    "\n",
    "#EoT: pt(Ecal)/pt(Tot)\n",
    "nLayerEcal= 14\n",
    "sumE=0\n",
    "\n",
    "sumT=0\n",
    "for i in range(n_layers):\n",
    "    #print('Totlayer_{}'.format(i))\n",
    "    sumT+=df_cut['layer_{}'.format(i)]\n",
    "    \n",
    "    \n",
    "for i in range(1,nLayerEcal+1):\n",
    "    #print('Ecal layer_{}'.format(i))\n",
    "    sumE+=df_cut['layer_{}'.format(i)]\n",
    "df_cut['EoT']=sumE/sumT\n",
    "\n",
    "#fraction of pt in first x  layers and last x layers\n",
    "maxfirst = 5\n",
    "maxlast=10\n",
    "\n",
    "for n in range(1,maxfirst+1):\n",
    "    Sum=0\n",
    "    for i in range(1,n+1):\n",
    "        Sum+=df_cut['layer_{}'.format(i)]\n",
    "    df_cut['first_{}'.format(n)]= Sum/sumT\n",
    "    \n",
    "for n in range(1,maxlast):\n",
    "    Sum=0\n",
    "    #print(n, ' layers')\n",
    "    for i in range(n_layers - n, n_layers):\n",
    "        #print(n,i)\n",
    "        Sum+=df_cut['layer_{}'.format(i)]\n",
    "    #print('last_{}'.format(n), sum/sumT)\n",
    "    df_cut['last_{}'.format(n)]= Sum/sumT\n",
    "    \n",
    "    \n",
    "\n",
    "for n in range(1,maxfirst+1):\n",
    "    Sum=0\n",
    "    for i in range(1,n+1):\n",
    "        #print('layer_{}'.format(nLayerEcal+i))\n",
    "        Sum+=df_cut['layer_{}'.format(nLayerEcal+i)]\n",
    "    df_cut['firstHcal_{}'.format(n)]= Sum/sumT\n",
    "    \n",
    "    \n",
    "# Emaxx : pt in x layers around Elec max layer (5)\n",
    "maxpos=5\n",
    "\n",
    "#print('emax_1')\n",
    "Sum=0\n",
    "Sum+=df_cut['layer_{}'.format(maxpos)]\n",
    "df_cut['Emax_1']=Sum/sumT\n",
    " \n",
    "#print('emax_2L')\n",
    "Sum=0\n",
    "for i in range(maxpos-1, maxpos+1):\n",
    "    #print(i)\n",
    "    Sum+=df_cut['layer_{}'.format(i)]\n",
    "    df_cut['Emax_2G']=Sum/sumT\n",
    "    \n",
    "#print('emax_2R')\n",
    "Sum=0\n",
    "for i in range(maxpos, maxpos+2):\n",
    "    #print(i)\n",
    "    Sum+=df_cut['layer_{}'.format(i)]\n",
    "    df_cut['Emax_2R']=Sum/sumT\n",
    "    \n",
    "#print('emax_3')\n",
    "Sum=0\n",
    "for i in range(maxpos-1, maxpos+2):\n",
    "    #print(i)\n",
    "    Sum+=df_cut['layer_{}'.format(i)]\n",
    "    df_cut['Emax_3']=Sum/sumT\n",
    "#print('emax_4G')\n",
    "Sum=0\n",
    "for i in range(maxpos-2, maxpos+2):\n",
    "    #print(i)\n",
    "    Sum+=df_cut['layer_{}'.format(i)]\n",
    "    df_cut['Emax_4L']=Sum/sumT\n",
    "#print('emax_4R')\n",
    "Sum=0\n",
    "for i in range(maxpos-1, maxpos+3):\n",
    "    #print(i)\n",
    "    Sum+=df_cut['layer_{}'.format(i)]\n",
    "    df_cut['Emax_4R']=Sum/sumT\n",
    "#print('emax_5')\n",
    "Sum=0\n",
    "for i in range(maxpos-2, maxpos+3):\n",
    "    #print(i)\n",
    "    Sum+=df_cut['layer_{}'.format(i)]\n",
    "    df_cut['Emax_5']=Sum/sumT\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 53s, sys: 6.08 s, total: 2min 59s\n",
      "Wall time: 2min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#do bitmaps with group\n",
    "def ebm0(x):\n",
    "    ebm=[]\n",
    "    thr=0\n",
    "    for i in range(1,nLayerEcal+1):\n",
    "        #print(float(x.layer_pt[i])>thr)\n",
    "        ebm.append(int(float(x.layer_pt[i])>thr)) \n",
    "    #print(ebm)\n",
    "    return(np.array(ebm).dot(2**np.arange(len(ebm))[::-1]))\n",
    "def ebm1(x):\n",
    "    ebm=[]\n",
    "    thr=1\n",
    "    for i in range(1,nLayerEcal+1):\n",
    "        #print(float(x.layer_pt[i])>thr)\n",
    "        ebm.append(int(float(x.layer_pt[i])>thr)) \n",
    "    #print(ebm)\n",
    "    return(np.array(ebm).dot(2**np.arange(len(ebm))[::-1]))\n",
    "def hbm(x):\n",
    "    ebm=[]\n",
    "    thr=1\n",
    "    for i in range(nLayerEcal+1,n_layers):\n",
    "        #print(float(x.layer_pt[i])>thr)\n",
    "        ebm.append(int(float(x.layer_pt[i])>thr)) \n",
    "    #print(ebm)\n",
    "    return(np.array(ebm).dot(2**np.arange(len(ebm))[::-1]))\n",
    "        \n",
    "df_cut['ebm0']=df_cut.apply(ebm0, axis=1)\n",
    "df_cut['ebm1']=df_cut.apply(ebm1, axis=1)\n",
    "df_cut['hbm']=df_cut.apply(hbm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 2min 55s, sys: 6.07 s, total: 3min 1s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#do bitmaps with group\n",
    "def reverse_ebm0(x):\n",
    "    ebm=[]\n",
    "    thr=0\n",
    "    for i in range(1,nLayerEcal+1):\n",
    "        #print(float(x.layer_pt[i])>thr)\n",
    "        ebm.append(int(float(x.layer_pt[i])>thr)) \n",
    "    #print(ebm)\n",
    "    return(np.array(ebm).dot(2**np.arange(len(ebm))[::-1]))\n",
    "def reverse_ebm1(x):\n",
    "    ebm=[]\n",
    "    thr=1\n",
    "    for i in range(1,nLayerEcal+1):\n",
    "        #print(float(x.layer_pt[i])>thr)\n",
    "        ebm.append(int(float(x.layer_pt[i])>thr)) \n",
    "    #print(ebm)\n",
    "    return(np.array(ebm).dot(2**np.arange(len(ebm))[::-1]))\n",
    "def reverse_hbm(x):\n",
    "    ebm=[]\n",
    "    thr=1\n",
    "    for i in range(nLayerEcal+1,n_layers):\n",
    "        #print(float(x.layer_pt[i])>thr)\n",
    "        ebm.append(int(float(x.layer_pt[i])>thr)) \n",
    "    #print(ebm)\n",
    "    return(np.array(ebm).dot(2**np.arange(len(ebm))[::-1]))\n",
    "        \n",
    "df_cut['reverse_ebm0']=df_cut.apply(reverse_ebm0, axis=1)\n",
    "df_cut['reverse_ebm1']=df_cut.apply(reverse_ebm1, axis=1)\n",
    "df_cut['reverse_hbm']=df_cut.apply(reverse_hbm, axis=1)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization\n",
    "\n",
    "def quantize(feat, nbits, method): #feat is the feature to quantize, nbist the number of bits, method: 'uniform' or 'percentile'\n",
    "    nbins= 2**nbits\n",
    "    fmin=feat.min()\n",
    "    fmax=feat.max()\n",
    "    if method == 'uniform':\n",
    "        bins=np.linspace(fmin, fmax, nbins+1)\n",
    "    elif method == 'percentile':\n",
    "        bins=[np.percentile(feat, n) for n in np.linspace(0,100,nbins+1)]\n",
    "    else :\n",
    "        print('Error: Invalid method')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    return np.digitize(feat, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genpart_exeta', 'genpart_pt', 'cl3d_pt', 'cl3d_eta', 'cl3d_showerlength', 'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_seetot', 'cl3d_seemax', 'cl3d_spptot', 'cl3d_sppmax', 'cl3d_szz', 'cl3d_srrtot', 'cl3d_srrmax', 'cl3d_srrmean', 'cl3d_emaxe', 'cl3d_hoe', 'cl3d_meanz', 'cl3d_layer10', 'cl3d_layer50', 'cl3d_layer90', 'cl3d_ntc67', 'cl3d_ntc90', 'matches', 'sample', 'abseta', 'layer_pt', 'layer_0', 'layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15', 'layer_16', 'layer_17', 'layer_18', 'layer_19', 'layer_20', 'layer_21', 'layer_22', 'layer_23', 'layer_24', 'layer_25', 'layer_26', 'layer_27', 'layer_28', 'layer_29', 'layer_30', 'layer_31', 'layer_32', 'layer_33', 'layer_34', 'layer_35', 'varee', 'varpp', 'varzz', 'varrr', 'EoT', 'first_1', 'first_2', 'first_3', 'first_4', 'first_5', 'last_1', 'last_2', 'last_3', 'last_4', 'last_5', 'last_6', 'last_7', 'last_8', 'last_9', 'firstHcal_1', 'firstHcal_2', 'firstHcal_3', 'firstHcal_4', 'firstHcal_5', 'Emax_1', 'Emax_2G', 'Emax_2R', 'Emax_3', 'Emax_4L', 'Emax_4R', 'Emax_5', 'ebm0', 'ebm1', 'hbm', 'reverse_ebm0', 'reverse_ebm1', 'reverse_hbm']\n"
     ]
    }
   ],
   "source": [
    "print(df_cut.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot features distrib\n",
    "plot= False\n",
    "if plot:\n",
    "    var_list= ['cl3d_showerlength', 'cl3d_coreshowerlength', 'cl3d_firstlayer',\n",
    "           'cl3d_maxlayer', 'cl3d_seetot', 'cl3d_spptot',\n",
    "           'cl3d_szz', 'cl3d_srrtot',\n",
    "           'cl3d_emaxe', 'cl3d_hoe', 'cl3d_meanz', 'cl3d_layer10', 'cl3d_layer50',\n",
    "           'cl3d_layer90', 'cl3d_ntc67', 'cl3d_ntc90', \n",
    "           'abseta', 'varee', 'varpp', 'varzz', 'varrr', 'EoT', 'first_1', 'first_2',\n",
    "            'first_3', 'first_4', 'first_5', 'last_1', 'last_2', 'last_3', 'last_4',\n",
    "            'last_5', 'last_6', 'last_7', 'last_8', 'last_9', 'firstHcal_1', 'firstHcal_2',\n",
    "            'firstHcal_3', 'firstHcal_4', 'firstHcal_5', 'Emax_1', 'Emax_2G', 'Emax_2R', 'Emax_3',\n",
    "            'Emax_4L', 'Emax_4R', 'Emax_5', 'ebm0', 'ebm1', 'hbm', 'reverse_ebm0', 'reverse_ebm1', 'reverse_hbm']\n",
    "\n",
    "    nbins=100\n",
    "    binmin=[1, 1, 1, 1, 0, 0, 0.0, 0.0, 0., -1.0, 328.15726, 0., 1.5, 4.004325, 0.67, 0.9, 1.6, 0, 0, 0.0, 0.0,\n",
    "     0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,0,0,0,0,0,0]\n",
    "    binmax=[50.0, 36.0, 34.0, 50.0, 0.09, 0.075, 90, 0.01, 1, 5, 500, 36, 36, 36, 80, 200, 2.9, 0.007,\n",
    "         0.005, 800, 0.0001, 1.0, 0.01, 0.03, 0.15, 0.5, 0.5,0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3, \n",
    "            0.3, 0.3,0.4, 0.6, 0.6,0.25,0.62,0.9]\n",
    "    \n",
    "    os.makedirs(workdir + '/distrib', exist_ok=True)\n",
    "    for i,feat in enumerate(var_list):\n",
    "        if i in [0,1,2,3]:\n",
    "            nbins=36\n",
    "        else: nbins= 100\n",
    "        #print(feat, binmin[i], binmax[i])\n",
    "        bins=np.linspace(binmin[i],binmax[i],nbins)\n",
    "        plt.figure(figsize=(12,7))\n",
    "        plt.hist(df_cut[feat][df_cut['sample']=='elec'], bins=bins, label='elec', density=True, histtype='step')\n",
    "        plt.hist(df_cut[feat][df_cut['sample']=='pion'],bins=bins, label='pions', density=True, histtype='step')\n",
    "        plt.hist(df_cut[feat][df_cut['sample']=='PU'],bins=bins, label='PU', density=True, histtype='step')\n",
    "        plt.title(\"{} distribution \".format(feat))\n",
    "        #plt.yscale('log')\n",
    "        #plt.xlim(0,36)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(workdir+'/distrib/{}.png'.format(feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot longitudinal profile: mean fraction of elec pt / layer\n",
    "frac={}\n",
    "for n in range(n_layers):\n",
    "    frac[n]=df_cut[df_cut['sample']==\"elec\"]['layer_{}'.format(n)]/df_cut[df_cut['sample']==\"elec\"]['cl3d_pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanfrac=[frac[n].mean() for n in range(n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 36 artists>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(n_layers),meanfrac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.model_selection import train_test_split\\n\\n\\nif feature_set == 1:\\n    features=['cl3d_showerlength', \\n       'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_szz',\\n       'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean']\\nelif feature_set ==2:\\n    features=[ 'cl3d_showerlength',\\n       'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_szz',\\n       'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_layer10',\\n        'cl3d_layer50','cl3d_layer90','cl3d_ntc67','cl3d_ntc90', 'cl3d_hoe','cl3d_meanz',\\n        'cl3d_emaxe' ]\\nelif feature_set ==3:\\n    features = [ 'cl3d_showerlength',\\n       'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_szz',\\n       'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_layer10',\\n        'cl3d_layer50','cl3d_layer90','cl3d_ntc67','cl3d_ntc90', 'cl3d_hoe','cl3d_meanz',\\n        'cl3d_emaxe' ]\\n    features=features+layer_columns\\nelif feature_set== 4:\\n    features=[ 'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_hoe','cl3d_meanz',\\n        'cl3d_emaxe','cl3d_ntc67','cl3d_ntc90',]\\n    features=features+layer_columns\\nelif feature_set==5:\\n    features=['cl3d_showerlength',\\n 'cl3d_coreshowerlength',\\n 'cl3d_firstlayer',\\n 'cl3d_maxlayer',\\n 'cl3d_seetot',\\n 'cl3d_seemax',\\n 'cl3d_spptot',\\n 'cl3d_sppmax',\\n 'cl3d_szz',\\n 'cl3d_srrtot',\\n 'cl3d_srrmax',\\n 'cl3d_srrmean',\\n 'cl3d_emaxe',\\n 'cl3d_hoe',\\n 'cl3d_meanz',\\n 'cl3d_layer10',\\n 'cl3d_layer50',\\n 'cl3d_layer90',\\n 'cl3d_ntc67',\\n 'cl3d_ntc90']\\n    \\nelif feature_set ==6:\\n    features = ['cl3d_showerlength',\\n       'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_szz',\\n       'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_layer10',\\n        'cl3d_layer50','cl3d_layer90','cl3d_ntc67','cl3d_ntc90', 'cl3d_hoe','cl3d_meanz',\\n        'cl3d_emaxe' ]\\n    features=features+layer_columns\\nelif feature_set ==7:\\n    features = [ 'cl3d_showerlength',\\n       'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_szz',\\n       'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_layer10',\\n        'cl3d_layer50','cl3d_layer90','cl3d_ntc67','cl3d_ntc90', 'cl3d_hoe','cl3d_meanz',\\n        'cl3d_emaxe', 'cl3d_eta' ]\\n    features=features+layer_columns\\n   \\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if feature_set == 1:\n",
    "    features=['cl3d_showerlength', \n",
    "       'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_szz',\n",
    "       'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean']\n",
    "elif feature_set ==2:\n",
    "    features=[ 'cl3d_showerlength',\n",
    "       'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_szz',\n",
    "       'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_layer10',\n",
    "        'cl3d_layer50','cl3d_layer90','cl3d_ntc67','cl3d_ntc90', 'cl3d_hoe','cl3d_meanz',\n",
    "        'cl3d_emaxe' ]\n",
    "elif feature_set ==3:\n",
    "    features = [ 'cl3d_showerlength',\n",
    "       'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_szz',\n",
    "       'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_layer10',\n",
    "        'cl3d_layer50','cl3d_layer90','cl3d_ntc67','cl3d_ntc90', 'cl3d_hoe','cl3d_meanz',\n",
    "        'cl3d_emaxe' ]\n",
    "    features=features+layer_columns\n",
    "elif feature_set== 4:\n",
    "    features=[ 'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_hoe','cl3d_meanz',\n",
    "        'cl3d_emaxe','cl3d_ntc67','cl3d_ntc90',]\n",
    "    features=features+layer_columns\n",
    "elif feature_set==5:\n",
    "    features=['cl3d_showerlength',\n",
    " 'cl3d_coreshowerlength',\n",
    " 'cl3d_firstlayer',\n",
    " 'cl3d_maxlayer',\n",
    " 'cl3d_seetot',\n",
    " 'cl3d_seemax',\n",
    " 'cl3d_spptot',\n",
    " 'cl3d_sppmax',\n",
    " 'cl3d_szz',\n",
    " 'cl3d_srrtot',\n",
    " 'cl3d_srrmax',\n",
    " 'cl3d_srrmean',\n",
    " 'cl3d_emaxe',\n",
    " 'cl3d_hoe',\n",
    " 'cl3d_meanz',\n",
    " 'cl3d_layer10',\n",
    " 'cl3d_layer50',\n",
    " 'cl3d_layer90',\n",
    " 'cl3d_ntc67',\n",
    " 'cl3d_ntc90']\n",
    "    \n",
    "elif feature_set ==6:\n",
    "    features = ['cl3d_showerlength',\n",
    "       'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_szz',\n",
    "       'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_layer10',\n",
    "        'cl3d_layer50','cl3d_layer90','cl3d_ntc67','cl3d_ntc90', 'cl3d_hoe','cl3d_meanz',\n",
    "        'cl3d_emaxe' ]\n",
    "    features=features+layer_columns\n",
    "elif feature_set ==7:\n",
    "    features = [ 'cl3d_showerlength',\n",
    "       'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_szz',\n",
    "       'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_layer10',\n",
    "        'cl3d_layer50','cl3d_layer90','cl3d_ntc67','cl3d_ntc90', 'cl3d_hoe','cl3d_meanz',\n",
    "        'cl3d_emaxe', 'cl3d_eta' ]\n",
    "    features=features+layer_columns\n",
    "   \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if feature_set == 1:\n",
    "    name = 'minimal w/ var'\n",
    "    features = ['abseta', 'cl3d_firstlayer', 'varee', 'varpp', 'varzz', 'varrr',\n",
    "                'cl3d_meanz','cl3d_showerlength','cl3d_coreshowerlength'] #hoe/maxlayer\n",
    "if feature_set == 2:\n",
    "    name = 'baseline w/ var'\n",
    "    features =  ['cl3d_showerlength','cl3d_coreshowerlength','cl3d_firstlayer','cl3d_maxlayer','varee', 'varzz',\n",
    "    'varpp', 'varrr',  'cl3d_emaxe', 'cl3d_hoe', 'cl3d_meanz', 'cl3d_layer10', 'cl3d_layer50',\n",
    "    'cl3d_layer90', 'cl3d_ntc67', 'cl3d_ntc90', 'abseta']\n",
    "\n",
    "if feature_set == 3:\n",
    "    name = 'minimal w/ var & EoT'\n",
    "    features = ['abseta', 'cl3d_firstlayer', 'varee', 'varpp', 'varzz', 'varrr',\n",
    "                 'cl3d_meanz', 'EoT', 'cl3d_showerlength','cl3d_coreshowerlength'] #hoe/maxlayer\n",
    "\n",
    "if feature_set == 4:\n",
    "    name = 'minimal w/ var & EoT & layervar & bitmap '\n",
    "    features = ['abseta', 'cl3d_firstlayer', 'varee', 'varpp', 'varzz', 'varrr',\n",
    "                 'cl3d_meanz','cl3d_showerlength','cl3d_coreshowerlength', 'EoT', 'first_1', 'first_2', 'first_3', 'first_4', 'first_5', 'last_1',\n",
    "       'last_2', 'last_3', 'last_4', 'last_5', 'last_6', 'last_7', 'last_8',\n",
    "       'last_9', 'firstHcal_1', 'firstHcal_2', 'firstHcal_3', 'firstHcal_4',\n",
    "       'firstHcal_5', 'firstHcal_6', 'Emax_1', 'Emax_2','Emax_3', 'ebm0', 'ebm1', 'hbm', 'reverse_ebm0', 'reverse_ebm1', 'reverse_hbm']\n",
    "\n",
    "\n",
    "if feature_set == 5:\n",
    "    name = 'minimal w/ var & best pions '\n",
    "    features = ['abseta', 'cl3d_firstlayer', 'varee', 'varpp', 'varzz', 'varrr',\n",
    "                 'cl3d_meanz', 'cl3d_showerlength','cl3d_coreshowerlength','EoT',\n",
    "                'Emax_3', 'ebm1', 'first_4', 'first_5', 'first_3', 'Emax_2']\n",
    "    \n",
    "        \n",
    "if feature_set == 6:\n",
    "    name = 'minimal w/ var & best PU '\n",
    "    features = ['abseta', 'cl3d_firstlayer', 'varee', 'varpp', 'varzz', 'varrr',\n",
    "                 'cl3d_meanz', 'cl3d_showerlength','cl3d_coreshowerlength', 'EoT',\n",
    "                'firstHcal_6', 'Emax_3', 'ebm1', 'Emax_1', 'first_5', 'Emax_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chose signal and background\n",
    "#weight calc\n",
    "\n",
    "df_cut['signal']= 5\n",
    "df_cut['weight']=0\n",
    "df_cut['signal'][df_cut['sample']=='elec']= 1\n",
    "df_cut['signal'][df_cut['sample']!='elec'] = 0\n",
    "\n",
    "if bkg == 'pions':\n",
    "\n",
    "    data=df_cut[df_cut['sample']!='PU']\n",
    "    tot_event=len(data)\n",
    "    sig_event=len(data[df['sample']=='elec'])\n",
    "    bkg_event=len(data[df['sample']=='pion'])\n",
    "    ws=tot_event/sig_event\n",
    "    wb=tot_event/bkg_event\n",
    "    #print(ws)\n",
    "    data['weight'][data['signal']==0]=wb\n",
    "    #print(np.unique(data['weight']))\n",
    "\n",
    "    #print(wb)\n",
    "    data['weight'][data['signal']==1]=ws\n",
    "    #print(np.unique(data['weight']))\n",
    "\n",
    "elif bkg == 'PU':\n",
    "    data=df_cut[df_cut['sample']!='pions']\n",
    "    tot_event=len(data)\n",
    "    sig_event=len(data[df['sample']=='elec'])\n",
    "    bkg_event=len(data[df['sample']=='PU'])\n",
    "    ws=tot_event/sig_event\n",
    "    wb=tot_event/bkg_event\n",
    "\n",
    "    data['weight'][data['signal']==0]=wb\n",
    "    data['weight'][data['signal']==1]=ws\n",
    "print('signal weight = {}, bkg weight ={}'.format(ws, wb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize features\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x=data[features]\n",
    "\n",
    "for key in features:\n",
    "        print(key, x[key].mean(), x[key].std())\n",
    "if standardize==  \"_Standardized\":\n",
    "    X=scaler.fit_transform(x)\n",
    "else :X = x\n",
    "pickle.dump(scaler, open(f\"{res_dir}/scaler_model.pkl\", \"wb\"))\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(X, columns=features), data[['signal','weight','cl3d_eta','cl3d_pt', 'genpart_pt', 'genpart_exeta']], test_size=0.2,random_state=42)\n",
    "\n",
    "train= xgb.DMatrix(data=X_train,label=y_train['signal'], feature_names=features, weight=y_train['weight'])\n",
    "test= xgb.DMatrix(data=X_test,label=y_test['signal'],feature_names=features, weight=y_test['weight'])\n",
    "\n",
    "#del(df_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "opti=False\n",
    "if opti ==True:\n",
    "    param = {\n",
    "        # Parameters that we are going to tune.\n",
    "        'nthread':16,\n",
    "        'max_depth':4,\n",
    "        'eta':0.05,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        # Other parameters\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric': 'auc', \n",
    "    }\n",
    "\n",
    "\n",
    "    #use gpu if available\n",
    "    gpu=tf.test.is_gpu_available(\n",
    "        cuda_only=False, min_cuda_compute_capability=None\n",
    "    )\n",
    "    if gpu==True:\n",
    "        print('gpu available')\n",
    "        param['tree_method']='gpu_hist'\n",
    "    else: print('no gpu found')\n",
    "\n",
    "\n",
    "\n",
    "    num_boost_round=200\n",
    "    es_rounds=5\n",
    "    #watchlist=\n",
    "    progress={}\n",
    "\n",
    "    metrics={'logloss','auc'} #last one used for es\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # grid search:\n",
    "    gridsearch_params = [\n",
    "        (max_depth, eta, l1, l2)\n",
    "        for max_depth in [4,5,6,8]\n",
    "        for eta in [0.01,0.05,0.1,0.2,0.3]\n",
    "        for l1 in [0.01, 0.1,1,10] #alpha, def= 0\n",
    "        for l2 in [0.01, 0.1,1,10] #lambda, def=1\n",
    "    ]\n",
    "    tot_iter=len(gridsearch_params)\n",
    "    n_iter=0\n",
    "    max_score = 0\n",
    "    best_params = None\n",
    "    for max_depth, eta, l1, l2 in gridsearch_params:\n",
    "        n_iter+=1\n",
    "        print(\"CV {}/{} with max_depth={}, eta={}, l1={}, l2={}\".format(n_iter, tot_iter,\n",
    "                                 max_depth, eta, l1, l2))\n",
    "        # Update our parameters\n",
    "        param['max_depth'] = max_depth\n",
    "        param['eta'] = eta\n",
    "        param['alpha']= l1\n",
    "        param['lambda']=l2\n",
    "        # Run CV\n",
    "        cv_results = xgb.cv(\n",
    "            param,\n",
    "            train,\n",
    "            num_boost_round=num_boost_round,\n",
    "            seed=42,\n",
    "            nfold=5,\n",
    "            metrics={'auc'},\n",
    "            early_stopping_rounds=es_rounds\n",
    "        )\n",
    "        # Update best MAE\n",
    "        alpha=1\n",
    "        mean_score = (cv_results['test-auc-mean']-alpha*abs(cv_results['test-auc-mean']-cv_results['train-auc-mean'])).max()\n",
    "        boost_rounds = (cv_results['test-auc-mean']-alpha*abs(cv_results['test-auc-mean']-cv_results['train-auc-mean'])).argmax()\n",
    "        print(\"\\tScore {} in {} rounds\".format(mean_score, boost_rounds))\n",
    "        if mean_score > max_score:\n",
    "            max_score = mean_score\n",
    "            best_params = (max_depth,eta, l1, l2)\n",
    "    print(\"Best params: {},  Score: {}\".format(best_params, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'nthread' : 8,\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':4,\n",
    "    'eta':0.2,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    # Other parameters\n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric': 'auc', \n",
    "    'alpha':10,\n",
    "    'lambda':10,\n",
    "}\n",
    "\n",
    "#use gpu if available\n",
    "gpu=tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")\n",
    "if gpu==True:\n",
    "    param['tree_method']='gpu_hist'\n",
    "\n",
    "num_boost_round=200\n",
    "es_rounds=5\n",
    "\n",
    "metrics={'logloss','auc'} #last one used for es\n",
    "# set up cross validation:\n",
    "print('beginning cv')\n",
    "cv_results = xgb.cv(\n",
    "    param,\n",
    "    train,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=40,\n",
    "    nfold=5,\n",
    "    metrics=metrics,\n",
    "    #feval=SoverB,\n",
    "    early_stopping_rounds=es_rounds,\n",
    "    shuffle=True,\n",
    "    verbose_eval=2\n",
    ")\n",
    "\n",
    "## we define best score as the best test_auc penalized by the gap between train and test auc\n",
    "alpha=1 #(penalization)\n",
    "\n",
    "best_score=(cv_results['test-auc-mean']-alpha*abs(cv_results['test-auc-mean']-cv_results['train-auc-mean'])).max()\n",
    "boost_rounds = (cv_results['test-auc-mean']-alpha*abs(cv_results['test-auc-mean']-cv_results['train-auc-mean'])).argmax()\n",
    "print('best score:{} reached in {} rounds'.format(best_score, boost_rounds))\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "    \n",
    "    plt.figure(figsize=(12,7))\n",
    "    x_axis=range(0,len(cv_results))\n",
    "    plt.errorbar(x=x_axis,y=cv_results['train-{}-mean'.format(metric)],xerr=None, yerr=cv_results['train-{}-std'.format(metric)], label='Train')\n",
    "    plt.errorbar(x=x_axis,y=cv_results['test-{}-mean'.format(metric)],xerr=None, yerr=cv_results['test-{}-std'.format(metric)], label='Test')\n",
    "    plt.plot(boost_rounds, cv_results['test-{}-mean'.format(metric)][boost_rounds], 'b+', label='best', markersize=16)\n",
    "    #plt.vlines(x=boost_rounds, ymin=0, ymax=cv_results['train-{}-mean'.format(metric)].max(), label='best score')\n",
    "    plt.ylabel(metric)\n",
    "    #plt.ylim(0.4,0.9)\n",
    "    plt.xlabel('number of rounds')\n",
    "    plt.legend()\n",
    "    plt.savefig(res_dir+'/{}.png'.format(metric))\n",
    "\n",
    "#plot ratio train/test\n",
    "for metric in metrics:\n",
    "    \n",
    "    plt.figure(figsize=(12,7))\n",
    "    x_axis=range(0,len(cv_results))\n",
    "    ratio = (cv_results['train-{}-mean'.format(metric)]/cv_results['test-{}-mean'.format(metric)])\n",
    "    plt.plot(ratio, label='ratio train/test')\n",
    "    #plt.errorbar(x=x_axis,y=cv_results['test-{}-mean'.format(metric)],xerr=None, yerr=cv_results['test-{}-std'.format(metric)], label='Test')\n",
    "    plt.plot(boost_rounds, ratio[boost_rounds], 'b+', label='best', markersize=16)\n",
    "    #plt.vlines(x=boost_rounds, ymin=0, ymax=cv_results['train-{}-mean'.format(metric)].max(), label='best score')\n",
    "    plt.ylabel(metric)\n",
    "    #plt.ylim(0.8,1.2)\n",
    "    plt.xlabel('number of rounds')\n",
    "    plt.title('Ratio of train_{} / test_{}'.format(metric, metric))\n",
    "    plt.legend()\n",
    "    plt.savefig(res_dir+'/{}_ratio.png'.format(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now train and fit best parameters BDT\n",
    "\n",
    "\n",
    "\n",
    "print('Training on {} events, signal : {}, bkg: {}'.format(len(y_train),len(y_train[y_train['signal']==1]),len(y_train[y_train['signal']==0])))\n",
    "print('Testing on {} events, signal : {}, bkg: {}'.format(len(y_test),len(y_test[y_test['signal']==1]),len(y_test[y_test['signal']==0])))\n",
    "eval_result = {}\n",
    "BDT = xgb.train(\n",
    "    param,\n",
    "    train,\n",
    "    num_boost_round=boost_rounds,\n",
    "    #feval = SoverB,\n",
    "    evals=[(train, 'Train'),(test, \"Test\")],\n",
    "    evals_result = eval_result,\n",
    "    verbose_eval=10,\n",
    ")\n",
    "BDT.save_model(res_dir+'/model.model')\n",
    "print('model saved')\n",
    "predictions_BDT_test=BDT.predict(test)\n",
    "predictions_BDT_train=BDT.predict(train)\n",
    "y_test['BDT']=predictions_BDT_test\n",
    "#y_test['BDT_layer']=pred_PU_layer\n",
    "\n",
    "#kolmogorov test \n",
    "from scipy import stats\n",
    "ks_test_sig=stats.ks_2samp(predictions_BDT_test[y_test['signal']==1],predictions_BDT_train[y_train['signal']==1])\n",
    "ks_test_bkg=stats.ks_2samp(predictions_BDT_test[y_test['signal']==0],predictions_BDT_train[y_train['signal']==0])\n",
    "if ((ks_test_sig[1]>=0.05) & (ks_test_bkg[1]>=0.05)):  #pass test if pvalue > 5%\n",
    "    print('KS test passed for sig: cannot distinguish the 2 distributions (pvalue ={})'.format(ks_test_sig[1]))\n",
    "    print('KS test passed for bkg: cannot distinguish the 2 distributions (pvalue ={})'.format(ks_test_bkg[1]))\n",
    "else: \n",
    "    print('KS test failed (pvalue ={}),Sig distributions are too different, check overtraining '.format(ks_test_sig[1]))\n",
    "    print('KS test failed (pvalue ={}),Bkg distributions are too different, check overtraining '.format(ks_test_bkg[1]))\n",
    "    \n",
    "\n",
    "#plot AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "plt.figure(figsize=(10,10))\n",
    "fpr, tpr, threshold = roc_curve(y_test['signal'].astype('int32'),predictions_BDT_test, pos_label=1, sample_weight=y_test['weight'])\n",
    "fpr.sort()\n",
    "tpr.sort()\n",
    "roc_auc_test = auc(fpr, tpr)\n",
    "print (\"AUC Score (Test): {:4%}\".format(roc_auc_test))\n",
    "plt.plot(tpr,fpr, label =' AUC test = %0.4f' %(roc_auc_test))\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_train['signal'].astype('int32'),predictions_BDT_train, pos_label=1, sample_weight=y_train['weight'])\n",
    "fpr.sort()\n",
    "tpr.sort()\n",
    "roc_auc_train = auc(fpr, tpr)\n",
    "print (\"AUC Score (Test): {:4%}\".format(roc_auc_train))\n",
    "plt.plot(tpr,fpr, label =' AUC train = %0.4f' %(roc_auc_train))\n",
    "plt.yscale('log')\n",
    "plt.xlim(0.9,1.01)\n",
    "#plt.ylim(0.6,1.05)\n",
    "plt.xlabel('Signal efficiency')\n",
    "plt.ylabel('Background efficiency')\n",
    "plt.title('BDT ROC curve (log)')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(res_dir+'/ROC.png')\n",
    "#plot BDT ouput by sample\n",
    "plt.figure(figsize=(12,7))\n",
    "x=[]\n",
    "nbins=30\n",
    " \n",
    "plt.hist(predictions_BDT_test[y_test['signal']==1],bins=np.linspace(0,1,nbins), label='test_signal', density=True, histtype='step', color='C0')\n",
    "plt.hist(predictions_BDT_test[y_test['signal']==0],bins=np.linspace(0,1,nbins), label='test_bkg', density=True, histtype='step', color='C1')\n",
    "plt.hist(predictions_BDT_train[y_train['signal']==1],bins=np.linspace(0,1,nbins), label='train_signal', density=True, histtype='step', linestyle='dashed', color='C0')\n",
    "plt.hist(predictions_BDT_train[y_train['signal']==0],bins=np.linspace(0,1,nbins), label='train_bkg', density=True, histtype='step', linestyle='dashed', color='C1')\n",
    "plt.xlabel('BDT output')\n",
    "plt.ylabel('events')\n",
    "plt.title('BDT output, ks sig pvalue = {:.3f}, ks bkg pvalue = {:.3f}'.format(ks_test_sig[1], ks_test_bkg[1]))\n",
    "plt.legend(fontsize=12, loc = 'upper center')\n",
    "plt.savefig(res_dir+'/BDToutput_trainvstest.png')\n",
    "\n",
    "#plot importance\n",
    "#importance\n",
    "plt.figure(figsize=(15,10))\n",
    "ax=plt.subplot(111)\n",
    "xgb.plot_importance(BDT,ax,grid=False, importance_type='total_gain', show_values=False);\n",
    "plt.savefig(res_dir+'/importance_totalgain.png')\n",
    "    \n",
    "\n",
    "# save config:\n",
    "file_name=res_dir+'/BDTconfig.txt'\n",
    "with open(file_name, 'w') as f:\n",
    "    f.write('BDT config:   \\n ')\n",
    "    f.write('params = {} \\n n_rounds= {} \\n auc_train ={}\\n auc_test={} \\n ks_test_sig={} \\n ks_test_bkg = {}'.format(param, boost_rounds, roc_auc_train, roc_auc_test, ks_test_sig, ks_test_bkg))\n",
    "    f.write('input_list: {}\\n'.format(features))\n",
    "    f.write('Training on {} events, signal : {}, bkg: {}\\n'.format(len(y_train),len(y_train[y_train['signal']==1]),len(y_train[y_train['signal']==0])))\n",
    "    f.write('Testing on {} events, signal : {}, bkg: {}'.format(len(y_test),len(y_test[y_test['signal']==1]),len(y_test[y_test['signal']==0])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap values\n",
    "import shap\n",
    "# select a set of background examples to take an expectation over\n",
    "#background = X_train.to_numpy()[np.random.choice(X_train.shape[0], 1000, replace=False)]\n",
    "\n",
    "# explain predictions of the model on four images\n",
    "explainer = shap.Explainer(BDT)\n",
    "a= X_test[(y_test['signal']==1).values].sample(1000)\n",
    "b = X_test[(y_test['signal']==0).values].sample(1000)\n",
    "shap_df= pd.concat([a,b])\n",
    "\n",
    "#shap_values = explainer.shap_values(X_test.to_numpy()[np.random.choice(X_test.shape[0], 1000, replace=False)])\n",
    "shap_values = explainer.shap_values(shap_df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature attributions\n",
    "#shap.image_plot(shap_values, -x_test[1:5])\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names=[val[5:] for val in shap_df.columns if val != 'abseta']\n",
    "col_names.append('abseta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "shap_df=pd.DataFrame(shap_values, columns=col_names)\n",
    "shap_mean=[]\n",
    "for column in shap_df.columns:\n",
    "    shap_mean.append(np.abs(shap_df[column]).mean())\n",
    "abs(shap_df).mean().sort_values().plot(kind='barh', figsize=(16,10))\n",
    "plt.savefig(res_dir+'/importance_shap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 30})\n",
    "plt.figure(figsize=(8,4))\n",
    "#shap.summary_plot(shap_values, X_test.to_numpy()[np.random.choice(X_test.shape[0], 1000, replace=False)], \n",
    "#                  features,max_display=99, show=False, alpha = 0.7 )\n",
    "\n",
    "shap.summary_plot(shap_values, shap_df.to_numpy() , col_names,max_display=99, show=False, alpha = 0.7 )\n",
    "plt.xlabel('Shap value', fontsize=16)\n",
    "plt.show()\n",
    "plt.savefig(res_dir+'/importance_violin.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df.columns=col_names\n",
    "explainer = shap.Explainer(BDT)\n",
    "shap_values = explainer(shap_df, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=explainer.shap_values(shap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test['BDT']=predictions_BDT_test\n",
    "#y_test['BDT_layer']=pred_PU_layer\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "score=[]\n",
    "thr=0.95\n",
    "\n",
    "\n",
    "\n",
    "#plot AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "plt.figure(figsize=(10,10))\n",
    "fpr, tpr, threshold = roc_curve(y_test['signal'].astype('int32'),predictions_BDT_test, pos_label=1,  sample_weight=y_test['weight'])\n",
    "fpr.sort()\n",
    "tpr.sort()\n",
    "roc_auc_test = auc(fpr, tpr)\n",
    "roc_test=pd.DataFrame({'tpr':tpr,'fpr':fpr, 'threshold':threshold})\n",
    "roc_cut_test=roc_test[roc_test['tpr']>thr];\n",
    "print (\"AUC Score (Test): {:4%}\".format(roc_auc_test))\n",
    "plt.plot(tpr,(1-fpr), label =' AUC test = %0.4f' %(roc_auc_test))\n",
    "\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_train['signal'].astype('int32'),predictions_BDT_train, pos_label=1,sample_weight=y_train['weight'])\n",
    "fpr.sort()\n",
    "tpr.sort()\n",
    "roc_auc_train = auc(fpr, tpr)\n",
    "roc_auc_train = auc(fpr, tpr)\n",
    "roc_train=pd.DataFrame({'tpr':tpr,'fpr':fpr, 'threshold':threshold})\n",
    "roc_cut_train=roc_train[roc_train['tpr']>thr];\n",
    "print (\"AUC Score (Train): {:4%}\".format(roc_auc_train))\n",
    "plt.plot(tpr,(1-fpr), label =' AUC train = %0.4f' %(roc_auc_train))\n",
    "plt.xlim(0.9,1.001)\n",
    "plt.yscale('log')\n",
    "plt.ylim(0.9,1.001)\n",
    "plt.xlabel('Signal efficiency')\n",
    "plt.ylabel('Background reduction')\n",
    "plt.title('BDT ROC curve')\n",
    "plt.axvline(x=thr, label='0.95 WP', c='k')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "print('score  = {}%'.format(np.min(roc_cut_test['fpr'])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc BDT cut\n",
    "thr=0.95\n",
    "BDT_cut=np.interp(thr,roc_test['tpr'], roc_test['threshold'])\n",
    "with open(file_name, 'a') as f:\n",
    "    f.write('BDT cut ={}  for tpr = {} and fpr {}\\n'.format( np.interp(thr,roc_test['tpr'], roc_test['threshold']), np.interp(0.95,roc_test['tpr'], roc_test['tpr']), np.interp(0.95,roc_test['tpr'], roc_test['fpr'])))\n",
    "\n",
    "print('BDT cut ={}  for tpr = {} and fpr {}'.format( np.interp(thr,roc_test['tpr'], roc_test['threshold']),\n",
    "                                                    np.interp(thr,roc_test['tpr'], roc_test['tpr']),\n",
    "                                                    np.interp(thr,roc_test['tpr'], roc_test['fpr'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binning y_test\n",
    "\n",
    "\n",
    "def calctp(group):\n",
    "    pos=len(group[(group['BDT']>BDT_cut) & (group['signal']==1)])\n",
    "    truepos= len(group[group['signal']==1])\n",
    "    \n",
    "    if truepos !=0 :return pos/truepos\n",
    "    else : return np.nan\n",
    "def calcfp(group):\n",
    "    fp = len(group[(group['BDT']>BDT_cut) & (group['signal']==0)])\n",
    "    trueneg = len(group[group['signal']==0])\n",
    "    if trueneg !=0: return fp/trueneg\n",
    "    else: return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CALCULATE ERROR BARS\n",
    "conf_level=0.99\n",
    "from scipy import stats\n",
    "#normal\n",
    "#def error(total, score, conf_level):\n",
    "#    alpha=(1-conf_level)/2\n",
    "#    sigma=np.sqrt(score*(1-score)/total)\n",
    "#    delta=np.abs(score-stats.norm.ppf(1-alpha,loc=score, scale=sigma))\n",
    "#    return delta\n",
    "\n",
    "#clopper pearson\n",
    "def error(total, score, conf_level):\n",
    "    alpha=(1-conf_level)/2\n",
    "    n=total\n",
    "    k=score*n\n",
    "    lo = score-stats.beta.ppf(alpha/2, k, n-k+1)\n",
    "    hi = stats.beta.ppf(1 - alpha/2, k+1, n-k)-score\n",
    "    return lo, hi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_bins=[0,5,10,15,20,30,40,60,80,100,120,140,160,180,200,220]\n",
    "eta_bins=[1.6,1.8,2.0,2.2,2.4,2.6,2.8,3.0]\n",
    "pt_points=[2.5,7.5,12.5,17.5,25,35,50,70,90,110,130,150,170,190,210]\n",
    "eta_points=[1.7,1.9,2.1,2.3,2.5,2.7,2.9]\n",
    "y_test['BDT']=predictions_BDT_test\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "plt.figure(figsize=(10,10))\n",
    "fpr, tpr, threshold = roc_curve(y_test['signal'].astype('int32'),predictions_BDT_test, pos_label=1,  sample_weight=y_test['weight'])\n",
    "fpr.sort()\n",
    "tpr.sort()\n",
    "roc_auc_test = auc(fpr, tpr)\n",
    "roc_test=pd.DataFrame({'tpr':tpr,'fpr':fpr, 'threshold':threshold})\n",
    "roc_cut_test=roc_test[roc_test['tpr']>thr];\n",
    "\n",
    "conf_level=0.99\n",
    "\n",
    "\n",
    "y_test['genpt_binned']=pd.cut(y_test['genpart_pt'], pt_bins)\n",
    "group=y_test.groupby('genpt_binned')\n",
    "tpr = group.apply(calctp)\n",
    "fpr= group.apply(calcfp)\n",
    "lo,hi=lo,hi=error(y_test.groupby('genpt_binned').size(), tpr, conf_level)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.errorbar(pt_points, tpr.values, yerr=[lo,hi], label ='signal efficiency ({} conf level)'.format(conf_level))\n",
    "plt.ylabel('signal efficiency')\n",
    "plt.xlabel('gen pt (GeV)')\n",
    "plt.title('total signal efficiency = {} ({:0.2f} BDT output cut)'.format(thr, BDT_cut))\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(res_dir+'/sigeffvspt_WP{}.png'.format(thr))\n",
    "\n",
    "\n",
    "y_test['cl3dpt_binned']=pd.cut(y_test['cl3d_pt'], pt_bins)\n",
    "group=y_test.groupby('cl3dpt_binned')\n",
    "tpr = group.apply(calctp)\n",
    "fpr= group.apply(calcfp)\n",
    "lo,hi=lo,hi=error(y_test.groupby('cl3dpt_binned').size(), fpr, conf_level)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.errorbar(pt_points, fpr.values, yerr=[lo,hi], label ='bkg efficiency ({} conf level)'.format(conf_level))\n",
    "plt.ylabel('bkg efficiency')\n",
    "plt.xlabel('cluster pt (GeV)')\n",
    "plt.title('total signal efficiency = {} ({:0.2f} BDT output cut)'.format(thr, BDT_cut))\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(res_dir+'/bkgeffvspt_WP{}.png'.format(thr))\n",
    "\n",
    "\n",
    "y_test['geneta_binned']=pd.cut(y_test['genpart_exeta'], eta_bins)\n",
    "group=y_test.groupby('geneta_binned')\n",
    "tpr = group.apply(calctp)\n",
    "fpr= group.apply(calcfp)\n",
    "lo,hi=lo,hi=error(y_test.groupby('geneta_binned').size(), fpr, conf_level)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.errorbar(eta_points, tpr.values, yerr=[lo,hi], label ='signal efficiency ({} conf level)'.format(conf_level))\n",
    "plt.ylabel('signal efficiency')\n",
    "plt.xlabel('gen eta ')\n",
    "plt.title('total signal efficiency = {} ({:0.2f} BDT output cut)'.format(thr, BDT_cut))\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(res_dir+'/sigeffvseta_WP{}.png'.format(thr))\n",
    "\n",
    "\n",
    "y_test['cl3deta_binned']=pd.cut(y_test['cl3d_eta'], eta_bins)\n",
    "group=y_test.groupby('cl3deta_binned')\n",
    "tpr = group.apply(calctp)\n",
    "fpr= group.apply(calcfp)\n",
    "lo,hi=lo,hi=error(y_test.groupby('cl3deta_binned').size(), fpr, conf_level)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.errorbar(eta_points, fpr.values, yerr=[lo,hi], label ='bkg efficiency ({} conf level)'.format(conf_level))\n",
    "plt.ylabel('bkg efficiency')\n",
    "plt.xlabel('cluster eta')\n",
    "plt.title('total signal efficiency = {} ({:0.2f} BDT output cut)'.format(thr, BDT_cut))\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(res_dir+'/bkgeffvseta_WP{}.png'.format(thr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.groupby('cl3dpt_binned').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#res pion BDT\n",
    "AUC Score (Test): 99.770155%\n",
    "AUC Score (Train): 99.903230%\n",
    "background rejection test = 0.9882920110192837, train = 0.994824909435915\n",
    "#res PU BDT\n",
    "AUC Score (Test): 99.901765%\n",
    "AUC Score (Train): 99.914897%\n",
    "background rejection test = 0.9994748699879739, train = 0.9995563555778215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bin in range(1,7):\n",
    "    print(bin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
